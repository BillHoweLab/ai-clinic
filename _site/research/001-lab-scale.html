<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Laboratory-Scale AI</title>
    <link rel="stylesheet" href="/assets/css/styles.css">
    <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" />
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Laboratory-Scale AI</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Laboratory-Scale AI" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The rapid proliferation of generative AI has raised questions about the competitiveness of lower-parameter, locally tunable, open-weight models relative to high-parameter, API-guarded, closed-weight models in terms of performance, domain adaptation, cost, and generalization. Centering under-resourced yet risk-intolerant settings in government, research, and healthcare, we see for-profit closed-weight models as incompatible with requirements for transparency, privacy, adaptability, and standards of evidence. Yet the performance penalty in using open-weight models, especially in low-data and low-resource settings, is unclear." />
<meta property="og:description" content="The rapid proliferation of generative AI has raised questions about the competitiveness of lower-parameter, locally tunable, open-weight models relative to high-parameter, API-guarded, closed-weight models in terms of performance, domain adaptation, cost, and generalization. Centering under-resourced yet risk-intolerant settings in government, research, and healthcare, we see for-profit closed-weight models as incompatible with requirements for transparency, privacy, adaptability, and standards of evidence. Yet the performance penalty in using open-weight models, especially in low-data and low-resource settings, is unclear." />
<link rel="canonical" href="http://localhost:4000/research/001-lab-scale.html" />
<meta property="og:url" content="http://localhost:4000/research/001-lab-scale.html" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-06-01T00:00:00-07:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Laboratory-Scale AI" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-06-01T00:00:00-07:00","datePublished":"2024-06-01T00:00:00-07:00","description":"The rapid proliferation of generative AI has raised questions about the competitiveness of lower-parameter, locally tunable, open-weight models relative to high-parameter, API-guarded, closed-weight models in terms of performance, domain adaptation, cost, and generalization. Centering under-resourced yet risk-intolerant settings in government, research, and healthcare, we see for-profit closed-weight models as incompatible with requirements for transparency, privacy, adaptability, and standards of evidence. Yet the performance penalty in using open-weight models, especially in low-data and low-resource settings, is unclear.","headline":"Laboratory-Scale AI","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/research/001-lab-scale.html"},"url":"http://localhost:4000/research/001-lab-scale.html"}</script>
<!-- End Jekyll SEO tag -->

  </head>
  <body>
    <nav class="nav_bar">
    
      <a href="/">Home</a>
    
      <a href="/clinic.html">Clinic</a>
    
      <a href="/research.html">Research</a>
    
      <a href="/our-workshops.html">Workshops</a>
    
      <a href="/people.html">People</a>
    
      <a href="/blog.html">Blog</a>
    
      <a href="/contact.html">Contact</a>
    
</nav>
    <h1>Laboratory-Scale AI</h1>
<h2>https://github.com/BillHoweLab/laboratory-scale-ai</h2>

<p>The rapid proliferation of generative AI has raised questions about the competitiveness of lower-parameter, locally tunable, open-weight models relative to high-parameter, API-guarded, closed-weight models in terms of performance, domain adaptation, cost, and generalization. Centering under-resourced yet risk-intolerant settings in government, research, and healthcare, we see for-profit closed-weight models as incompatible with requirements for transparency, privacy, adaptability, and standards of evidence. Yet the performance penalty in using open-weight models, especially in low-data and low-resource settings, is unclear.</p>

<p>We assess the feasibility of using smaller, open-weight models to replace GPT-4-Turbo in zero-shot, few-shot, and fine-tuned regimes, assuming access to only a single, low-cost GPU. We assess value-sensitive issues around bias, privacy, and abstention on three additional tasks relevant to those topics. We find that with relatively low effort, very low absolute monetary cost, and relatively little data for fine-tuning, small open-weight models can achieve competitive performance in domain-adapted tasks without sacrificing generality. We then run experiments considering practical issues in bias, privacy, and hallucination risk, finding that open models offer several benefits over closed models. We intend this work as a case study in understanding the opportunity cost of reproducibility and transparency over for-profit state-of-the-art zero shot performance, finding this cost to be marginal under realistic settings.</p>


  </body>
</html>