<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Mitigating Overconfidence in LLMs</title>
    <link rel="stylesheet" href="/assets/css/styles.css">
    <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" />
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Mitigating Overconfidence in LLMs</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Mitigating Overconfidence in LLMs" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Confidence estimation is a crucial area in machine learning, particularly with large language models (LLMs), which are prone to overconfidence, leading to inaccurate predictions, hallucinations, and impaired decision-making. As LLMs are increasingly integrated into real-world applications, overconfidence poses challenges for effective human-machine collaboration. We examine LLM overconfidence through the lens of human behavior, proposing a mechanism for understanding of how models exhibit overconfidence and how to mitigate its effects to improve LLM interpretability and calibration. Drawing on models of human overconfidence in cognitive and psychological research, we consider whether LLMs mirror human overconfidence patterns related to perceived task difficulty and comparisons with others. Our findings indicate that LLMs exhibit varied confidence patterns. Larger models, similar to humans, tend to overestimate their performance on challenging tasks and underestimate it on simpler ones, while small models display consistent overconfidence across all task levels. However, LLMs’ self-assessments are generally less sensitive to task difficulty than human estimates. We propose Answer-Free Confidence Estimation (AFCE), a method that reduces overconfidence by asking models for confidence scores on question sets without providing answers. This approach decouples confidence estimation from answer generation, significantly lowering overconfidence, particularly on challenging tasks. We then consider how LLMs’ self-assessment compares to their assessment of experts and laymen, providing insight into how LLMs place their own abilities, even though the actual accuracies between the two groups remains comparable. We aim to motivate psychology-grounded research for better confidence calibration in LLMs." />
<meta property="og:description" content="Confidence estimation is a crucial area in machine learning, particularly with large language models (LLMs), which are prone to overconfidence, leading to inaccurate predictions, hallucinations, and impaired decision-making. As LLMs are increasingly integrated into real-world applications, overconfidence poses challenges for effective human-machine collaboration. We examine LLM overconfidence through the lens of human behavior, proposing a mechanism for understanding of how models exhibit overconfidence and how to mitigate its effects to improve LLM interpretability and calibration. Drawing on models of human overconfidence in cognitive and psychological research, we consider whether LLMs mirror human overconfidence patterns related to perceived task difficulty and comparisons with others. Our findings indicate that LLMs exhibit varied confidence patterns. Larger models, similar to humans, tend to overestimate their performance on challenging tasks and underestimate it on simpler ones, while small models display consistent overconfidence across all task levels. However, LLMs’ self-assessments are generally less sensitive to task difficulty than human estimates. We propose Answer-Free Confidence Estimation (AFCE), a method that reduces overconfidence by asking models for confidence scores on question sets without providing answers. This approach decouples confidence estimation from answer generation, significantly lowering overconfidence, particularly on challenging tasks. We then consider how LLMs’ self-assessment compares to their assessment of experts and laymen, providing insight into how LLMs place their own abilities, even though the actual accuracies between the two groups remains comparable. We aim to motivate psychology-grounded research for better confidence calibration in LLMs." />
<link rel="canonical" href="http://localhost:4000/research/002-confidence.html" />
<meta property="og:url" content="http://localhost:4000/research/002-confidence.html" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-12-01T00:00:00-08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Mitigating Overconfidence in LLMs" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-12-01T00:00:00-08:00","datePublished":"2024-12-01T00:00:00-08:00","description":"Confidence estimation is a crucial area in machine learning, particularly with large language models (LLMs), which are prone to overconfidence, leading to inaccurate predictions, hallucinations, and impaired decision-making. As LLMs are increasingly integrated into real-world applications, overconfidence poses challenges for effective human-machine collaboration. We examine LLM overconfidence through the lens of human behavior, proposing a mechanism for understanding of how models exhibit overconfidence and how to mitigate its effects to improve LLM interpretability and calibration. Drawing on models of human overconfidence in cognitive and psychological research, we consider whether LLMs mirror human overconfidence patterns related to perceived task difficulty and comparisons with others. Our findings indicate that LLMs exhibit varied confidence patterns. Larger models, similar to humans, tend to overestimate their performance on challenging tasks and underestimate it on simpler ones, while small models display consistent overconfidence across all task levels. However, LLMs’ self-assessments are generally less sensitive to task difficulty than human estimates. We propose Answer-Free Confidence Estimation (AFCE), a method that reduces overconfidence by asking models for confidence scores on question sets without providing answers. This approach decouples confidence estimation from answer generation, significantly lowering overconfidence, particularly on challenging tasks. We then consider how LLMs’ self-assessment compares to their assessment of experts and laymen, providing insight into how LLMs place their own abilities, even though the actual accuracies between the two groups remains comparable. We aim to motivate psychology-grounded research for better confidence calibration in LLMs.","headline":"Mitigating Overconfidence in LLMs","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/research/002-confidence.html"},"url":"http://localhost:4000/research/002-confidence.html"}</script>
<!-- End Jekyll SEO tag -->

  </head>
  <body>
    <nav class="nav_bar">
    
      <a href="/">Home</a>
    
      <a href="/clinic.html">Clinic</a>
    
      <a href="/schedule.html">Office Hours</a>
    
      <a href="/projects.html">Projects</a>
    
      <a href="/our-workshops.html">Workshops</a>
    
      <a href="/people.html">People</a>
    
      <a href="/blog.html">Blog</a>
    
      <a href="/contact.html">Join Us</a>
    
</nav>
    <h1>Mitigating Overconfidence in LLMs</h1>
<div class="research_header">
    <div>NeurIPS 2024 Workshop on Behavioral ML</div>
    <div class="research_persons">Bingbing Wen, Chenjun Xu, Bin Han, Robert Wolfe, Lucy Lu Wang, and Bill Howe</div>
</div>

<div class="research_layout_row">
    <button class="research_paper_button"><a class="button_text" href=https://openreview.net/pdf?id=y9UdO5cmHs>Research Paper</a></button>
    <button class="research_code_button"><a class="button_text" href=>Research Code</a></button>
</div>

<div class="research_layout_row">
<div class="research_paper_content_card">
<h3 style="text-align:center">Paper Summary</h3>
<p>Confidence estimation is a crucial area in machine learning, particularly with large language models (LLMs), which are prone to overconfidence, leading to inaccurate predictions, hallucinations, and impaired decision-making. As LLMs are increasingly integrated into real-world applications, overconfidence poses challenges for effective human-machine collaboration. We examine LLM overconfidence through the lens of human behavior, proposing a mechanism for understanding of how models exhibit overconfidence and how to mitigate its effects to improve LLM interpretability and calibration. Drawing on models of human overconfidence in cognitive and psychological research, we consider whether LLMs mirror human overconfidence patterns related to perceived task difficulty and comparisons with others. Our findings indicate that LLMs exhibit varied confidence patterns. Larger models, similar to humans, tend to overestimate their performance on challenging tasks and underestimate it on simpler ones, while small models display consistent overconfidence across all task levels. However, LLMs’ self-assessments are generally less sensitive to task difficulty than human estimates. We propose Answer-Free Confidence Estimation (AFCE), a method that reduces overconfidence by asking models for confidence scores on question sets without providing answers. This approach decouples confidence estimation from answer generation, significantly lowering overconfidence, particularly on challenging tasks. We then consider how LLMs’ self-assessment compares to their assessment of experts and laymen, providing insight into how LLMs place their own abilities, even though the actual accuracies between the two groups remains comparable. We aim to motivate psychology-grounded research for better confidence calibration in LLMs.</p>

</div>
</div>
    <footer>
    <div class="footer-container">
        <div class="footer-section about">
            <h2>About The AI Clinic</h2>
            <p>The AI Clinic is an initiative of the UW iSchool to make language models more approachable in research.</p>
        </div>

        <div class="footer-section contact">
            <h2>Contact</h2>
            <p>Dr. Bill Howe: <a style="color:white" href='mailto:billhowe@uw.edu'>billhowe@uw.edu</a></p>
        </div>
    </div>
    <div class="footer-bottom">
        <p>&copy; 2025 The AI Clinic | All rights reserved.</p>
    </div>
</footer>

<!-- FontAwesome Icons -->
<script src="https://kit.fontawesome.com/a076d05399.js" crossorigin="anonymous"></script>

  </body>
</html>